// Chargement des dÃ©pendances
require("dotenv").config();
const express = require("express");
const bodyParser = require("body-parser");
const { Configuration, OpenAIApi } = require("openai");
const cors = require("cors");

const app = express();
const port = 3000;

// Configuration de l'API OpenAI
const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});
const openai = new OpenAIApi(configuration);

// Middleware
app.use(cors());
app.use(bodyParser.json());

// Route GET pour vÃ©rifier que le serveur fonctionne
app.get("/", (req, res) => {
  res.send("ğŸš€ Serveur NORR opÃ©rationnel !");
});

// Endpoint principal pour le chatbot
app.post("/api/chat", async (req, res) => {
  console.log("ğŸ“¥ RequÃªte reÃ§ue sur /api/chat :", req.body);

  if (!req.body.conversation || !Array.isArray(req.body.conversation)) {
    return res.status(400).json({ error: "â›” Le champ 'conversation' est requis et doit Ãªtre un tableau." });
  }

  // VÃ©rifier si la question concerne un enfant et si l'Ã¢ge est manquant
  const userMessage = req.body.conversation[req.body.conversation.length - 1].content.toLowerCase();
  const ageMentionnÃ© = /\b(\d+)\s?(an|ans)\b/.test(userMessage);

  if (userMessage.includes("mon enfant") || userMessage.includes("ma fille") || userMessage.includes("mon fils")) {
    if (!ageMentionnÃ©) {
      return res.json({ reply: "Quel est l'Ã¢ge de votre enfant pour que je puisse rÃ©pondre plus prÃ©cisÃ©ment ?" });
    }
  }

  try {
    const messages = [
      {
        role: "system",
        content: `
        Tu es **NORR**, un assistant parental bienveillant qui aide les parents en intÃ©grant des pratiques Ã©ducatives positives et spirituelles.
        
        ğŸ¯ **Tes inspirations** :
        - Tu t'appuies sur **Isabelle Filiozat** et **Emmanuelle Piquet** pour l'approche Ã©ducative et psychologique.
        - Tu intÃ¨gres aussi la vision spirituelle de **Lulumineuse**, en aidant les parents Ã  accompagner leurs enfants sur un chemin de lumiÃ¨re et de comprÃ©hension de soi.

        ğŸ“ **RÃ¨gles de rÃ©ponse** :
        - **Tes rÃ©ponses doivent Ãªtre bien structurÃ©es** : utilise des **titres en gras**, des **numÃ©ros en emojis** (1ï¸âƒ£, 2ï¸âƒ£...) et **des sauts de ligne entre chaque point**.
        - **RÃ©ponses concises et claires** (âš¡ **maximum 300 tokens**).
        - **Si tu es proche de la limite des 300 tokens**, **arrÃªte-toi naturellement et demande** :  
          ğŸ‘‰ *"Souhaitez-vous que je continue ?"*
        - **Si l'utilisateur rÃ©pond 'oui'**, continue **lÃ  oÃ¹ tu t'es arrÃªtÃ©** sans redemander s'il veut poursuivre.
      `,
      },
      ...req.body.conversation, 
    ];

    const completion = await openai.createChatCompletion({
      model: "gpt-4-turbo",
      messages: messages,
      max_tokens: 300,  // âš¡ Limite la rÃ©ponse pour accÃ©lÃ©rer le temps de rÃ©ponse
    });

    let fullReply = completion.data.choices[0].message.content;

    // âœ… Anticiper la coupure en ajoutant une question avant d'atteindre 300 tokens
    if (fullReply.length >= 280) {
        fullReply += " [...] Souhaitez-vous que je continue ?";
    }

    console.log("âœ… RÃ©ponse gÃ©nÃ©rÃ©e :", fullReply);

    res.json({ reply: fullReply });
  } catch (error) {
    console.error("âŒ Erreur OpenAI :", error.response ? error.response.data : error.message);
    res.status(500).json({ error: "Erreur serveur lors de la gÃ©nÃ©ration de la rÃ©ponse." });
  }
});

// ğŸš€ DÃ©marrage du serveur
app.listen(port, () => {
  console.log(`ğŸŒ Serveur NORR en cours d'exÃ©cution sur http://localhost:${port}`);
});













