// Chargement des d√©pendances
require("dotenv").config();
const express = require("express");
const bodyParser = require("body-parser");
const { Configuration, OpenAIApi } = require("openai");
const cors = require("cors");

const app = express();
const port = 3000;

// Configuration de l'API OpenAI
const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});
const openai = new OpenAIApi(configuration);

// Middleware
app.use(cors());
app.use(bodyParser.json());

// Route GET pour v√©rifier que le serveur fonctionne
app.get("/", (req, res) => {
  res.send("üöÄ Serveur NORR op√©rationnel !");
});

// Fonction de mise en forme des r√©ponses (sauts de ligne, emojis num√©rot√©s)
const formatResponse = (text) => {
  return text
    .replace(/(\d+)\./g, (match, number) => `\n\n${number}Ô∏è‚É£ **`) // Num√©ros en emoji + gras
    .replace(/\*\*(.*?)\*\*/g, "**$1**") // Assurer le gras des titres
    .replace(/\n/g, "<br>"); // Convertir les sauts de ligne en HTML
};

// Endpoint principal pour le chatbot
app.post("/api/chat", async (req, res) => {
  console.log("üì• Requ√™te re√ßue sur /api/chat :", req.body);

  if (!req.body.conversation || !Array.isArray(req.body.conversation)) {
    return res.status(400).json({ error: "‚õî Le champ 'conversation' est requis et doit √™tre un tableau." });
  }

  const lastUserMessage = req.body.conversation
    .filter(msg => msg.role === "user")
    .pop()?.content || "";

  const userAgeMatch = lastUserMessage.match(/\b(\d+)\s*(ans|an)\b/);
  const userAge = userAgeMatch ? parseInt(userAgeMatch[1]) : null;

  if (!userAge && /mon enfant|mon fils|ma fille/i.test(lastUserMessage)) {
    return res.json({ reply: "Quel est l'√¢ge de votre enfant pour que je puisse r√©pondre plus pr√©cis√©ment ?" });
  }

  try {
    const messages = [
      {
        role: "system",
        content: `
          Tu es NORR, un assistant parental chaleureux et compatissant.
          Tu es l√† pour aider les parents √† naviguer dans leurs d√©fis quotidiens avec bienveillance et clart√©.
          Tu t'appuies sur les travaux d'Isabelle Filiozat, Emmanuelle Piquet et Lulumineuse pour enrichir tes conseils avec des perspectives psychologiques et spirituelles.

          üéØ **Objectifs de ton discours :**
          - Reste **naturel et humain**, √©vite un ton trop acad√©mique ou m√©canique.
          - **Engage-toi √©motionnellement** : montre de l'empathie et fais sentir √† l'utilisateur qu'il est compris.
          - **Utilise un langage fluide et accessible** : √©vite les longues explications trop didactiques.
          - **Pose des questions pour inviter l'utilisateur √† interagir** plut√¥t que de donner une r√©ponse compl√®te d‚Äôun coup.
          
          ‚úÖ **Exemples de tournures naturelles** :
          - "Ah, c'est une situation d√©licate ! Je comprends que √ßa puisse √™tre frustrant..."
          - "Je vois, et vous avez d√©j√† essay√© quelque chose pour g√©rer √ßa ?"
          - "Un truc qui marche souvent, c‚Äôest..."
          - "Vous aimeriez explorer cette piste ensemble ?"

          ‚úÖ **R√®gles suppl√©mentaires :**
          - Si la r√©ponse risque d'√™tre trop longue, demande avant : "Souhaitez-vous que je d√©veloppe cette id√©e ?"
          - Si l'utilisateur donne un √¢ge approximatif (ex: "vers 5 ans"), demande un √¢ge pr√©cis.
          - Formate la r√©ponse avec des num√©ros en emoji (1Ô∏è‚É£, 2Ô∏è‚É£, etc.), mets les titres en **gras**, et ajoute des sauts de ligne clairs entre les paragraphes pour une lecture fluide.
        `,
      },
      ...req.body.conversation, 
    ];

    const completion = await openai.createChatCompletion({
      model: "gpt-4-turbo",
      messages: messages,
      max_tokens: 300,  // ‚ö° Limite la r√©ponse pour acc√©l√©rer le temps de r√©ponse
    });

    let fullReply = completion.data.choices[0].message.content;

    // ‚úÖ Anticipation si NORR approche la limite des tokens
    if (fullReply.length > 280 && !fullReply.includes("Souhaitez-vous que je d√©veloppe ?")) {
      fullReply += "\n\nüîπ Souhaitez-vous que je d√©veloppe ?";
    }

    // ‚úÖ Appliquer la mise en forme
    fullReply = formatResponse(fullReply);

    console.log("‚úÖ R√©ponse g√©n√©r√©e :", fullReply);
    res.json({ reply: fullReply });

  } catch (error) {
    console.error("‚ùå Erreur OpenAI :", error.response ? error.response.data : error.message);
    res.status(500).json({ error: "Erreur serveur lors de la g√©n√©ration de la r√©ponse." });
  }
});

// üöÄ D√©marrage du serveur
app.listen(port, () => {
  console.log(`üåç Serveur NORR en cours d'ex√©cution sur http://localhost:${port}`);
});














